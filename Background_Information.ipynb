{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "034483f8-1065-476f-b6ad-672179d98b78",
   "metadata": {},
   "source": [
    "## Quantum Computer Noise Fingerprints\n",
    "Quantum computers use quantum mechanical properties such as superposition and entanglement to perform computations. In some cases, these computations can far outperform classical computers. Quantum computers store information in quantum bits which can be in a state of 0, 1, or a superposition of both. After a circuit is run on a quantum computer, the output can only be seen by measuring the quantum bits. When a quantum bit is measured, the measurement returns either 0 or 1. The probability of either outcome depends on the quantum superpositon immediately prior to measurement. The probability of measuring 0 or 1 can be found by repeating the computation and recording the measurement result many times. When using a circuit with multiple quantum bits, the probabilities distribution is over all combinations of outcomes (for two quantum bits, those are 00, 01, 10, and 11). Here, the output of a quantum circuit refers to the probability distribution over the possible measurement outcomes found by running a quantum circuit.\n",
    "\n",
    "Quantum computers suffer from errors, also called noise. As a result of imperfect computation and measurement, the output of a quantum circuit is never exactly the same as the theory predicts. For example, if the expected output of a single quantum bit circuit is to measure 0 with 30% probability and 1 with 70% probability, the actual output might be to measure 0 with 32% probability and 1 with 68% probability. If that circuit was run again, the noise might be different leading to measuring 0 with 31% probability and 1 with 69% probability. The quantum computer noisy outputs dataset is a collection of outputs from various circuits run on different quantum computers. This dataset can help researchers to understand how noise affects quantum computers.\n",
    "\n",
    "The dataset contains the output data from running nine circuits 2000 times each on seven different quantum computers. The dataset was generated by running code found on the [Quantum Noise Fingerprint GitHub page](https://github.com/trianam/learningQuantumNoiseFingerprint) [1]. The quantum computers are Santiago, Lima, Quito, Bogota, Casablanca, Yorktown, and Athens and were built by IBM. The output from the quantum circuits used in this experiment consist of the probabilities of measuring four possible outcomes: 00, 01, 10, or 11. All of the numerical data points are probabilities, so they lie between 0 and 1. One row of the dataset consists of the four outcome probabilities for the nine circuits totalling 36 features and the output column containing the name of the quantum computer used the run the circuits.\n",
    "\n",
    "The quantum computers are each affected differently by noise and this leads to unique outcomes for every computer (and every circuit). By studying the distributions accross the nine circuits, a sort of “noise fingerprint” can be found which distinguishes the quantum computers from one another. If a suitable model can be learned, it will be possible to classify which quantum computer is in use just by studying the output of a circuit.\n",
    "\n",
    "**The classification problem is to label which computer was used to some circuits based on the input statistics about the probability outcomes. Here, quantum kernel estimation is used to train the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e82217-1e8a-4115-b2d6-b992bcfae3db",
   "metadata": {},
   "source": [
    "## Quantum Kernel Estimation\n",
    "In 2020, Liu, Arunachalam, and Temme established a \"rigorous quantum speed-up for supervised classification using a general-purpose quantum learning algorithm that only requires classical access to data\" [2]. The heart of the algorithm involves mapping classical data to a quantum feature space in a quantum version of the well-known \"kernel trick\" used in classical ML. Liu et. al. called this procedure *Quantum Kernel Estimation (QKE)* [2]. The quantum feature map is able to find complex patterns which are too hard for a classical computer to discover. Liu et. al. proved that their quantum classifier is guaranteed to achieve high accuracy for some classically intractable problems [2].\n",
    "\n",
    "### Classical Support Vector Machines and the Kernel Trick\n",
    "Support Vector Machine (SVM) is a classical ML method used mainly for binary classification problems. Given training data which is labelled as being in one of two classes, SVM finds a hyper-plane that separates the input data with the greatest gap between the two classes. Unfortunately, if the data is not linearly separable (i.e. no hyper-plane exists which separates the classes) the basic SVM algorithm does not work. \n",
    "\n",
    "Fortunately, data can be mapped into higher-dimensional space, via a function called a feature map, wherein it is linearly separable. A kernel function is an inner product used to determine the similarity of two points without actually knowing where they are in the higher-dimensional space. The kernel trick is way of running the algorithm on higher dimensional space without explicitly doing the feature map calculation. This is useful because SVM decides whether a hyper-plane is good only by finding the distance between points. The kernel can be described by (1), where $x$ and $y$ are input data points and $\\phi$ is the feature map. \n",
    "\n",
    "\\begin{equation}\n",
    "    k(x,y) = \\left\\langle \\phi(x), \\phi(y) \\right\\rangle\n",
    "    \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "Many valid inner products exist which can be used as kernels. The simple calculation in (1) finds the same output as calculating the whole list of features from the mapping $\\phi(x)$ and then taking the distance between those long vectors. \n",
    "\n",
    "### Quantum Kernel Trick\n",
    "QKE uses a feature map of the form in (2) where $x$ is an input data point of length $n$ and $k = n- t\\log n$ for some constant $t$ [2]. Classical feature maps take input data to higher-dimensional space to combat non-linearity, and similarly, QKE maps input data to quantum space to combat complexities that cannot be solved by classical computation. \n",
    "\n",
    "\\begin{equation}\n",
    "    |\\phi(x)\\rangle = \\frac{1}{\\sqrt{2^k}} \\sum_{i=0}^{2^k - 1} |x \\cdot g^i\\rangle\n",
    "    \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Just like the classical kernel trick, it suffices to calculate the inner product in (3) between the feature map outputs for different input data points $x_i$. QKE is a procedure which involves applying a two unitary operators $U(x_i)$ and $U(x_j)$ to the input state $|0\\rangle^{\\otimes n}$ and measuring the probability of the $0^n$ output [2]. This leads to an estimate of the inner product! \n",
    "\n",
    "\\begin{equation}\n",
    "     \\left|\\langle\\phi(x_i)|\\phi(x_j)\\rangle\\right|^2\n",
    "     \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "After carrying out QKE on a training set of $m$ samples, an $m \\times m$ quantum kernel matrix is created which is then the basis for the classical SVM algorithm [2]. QKE is the only subroutine that requires a quantum computer, while all other optimization steps can be performed classically. The hybrid quantum-classical solution of SVM-QKE introduced by Liu et. al. is demonstrably better than classical methods and can be used to solve classically intractable problems [2]!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1aa435-fbba-4154-8ca9-07ddec175763",
   "metadata": {},
   "source": [
    "## Sources\n",
    "[1] S. Martina, L. Buffoni, S. Gherardini, and F. Caruso. \"Learning the noise fingerprint of quantum devices,\" 2021. \n",
    "\n",
    "[2] Y. Liu, S. Arunachalam, and K. Temme, \"A rigorous and robust quantum speed-up in supervised machine learning,\" Nature Physics, vol. 17, no. 9, pp. 1013–1017, 2021. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
